---
title: Regresion_lineal_multiple
author: Nelson Sarmiento,Nayeli Ramon, Valeria Ulloa
format: html 
editor: visual
---

# Aprendizaje estad√≠stico

En general, la relaci√≥n de los valores de entrada y salida se puede escribir de la sifuiente forma:

$$ Y = f(X) + ‚àä $$

Esto se centra en la estimaci√≥n de f desde varios enfoques. Existen 2 razones principales para estimar f:

### Predicci√≥n

En este punto es necesario entender que un conjunto de entradas (x) est√° facilmente disponible mientras que las salidas (y) no son faciles de obtener ya que existe la posibilidad del error, si tomamos que el t√©rmino de error es un promedio 0 pordemos predecir siguiendo la f√≥rmula:

$$ Y' = f'(X) $$

f' : estimaci√≥n para f.

Y': predicci√≥n resultante para Y.

Por lo general, dentro de esta f√≥rmula la forma de f' no es tomada en cuenta siempre que produzca las predicciones precisas para Y.

La precisi√≥n de Y' depende de 2 errores el reducible y el irreducible.

-   Reducible: Este error se puede minimizar usando la t√©cnica de aprendizaje estad√≠stico, m√°s apropiada para estimar f.
-   Irreducible: Este error se debe a que Y tambi√©n en una funci√≥n de ‚àä.

### Inferencia

Dentro de la inferencia es necesario tomar en cuenta que nos interesa saber la forma exacta de f'. Esto se debe a que nos interesa conocer la asociaci√≥n entre cada variable y probabilidad a diferencia de la predicci√≥n que se interesa m√°s en la precisi√≥n del resultado.

## Estimar f

Una forma de estimar f es aplicar el m√©todo de aprendizaje estad√≠stico a los datos de entrenamiento, estos m√©todos se pueden caracterizar como param√©tricos y no param√©tricos:

-   Param√©tricos: se basa en modelos de 2 pasos, hacer la suposici√≥n de la forma funcional y utilizar el procedimeinto que use los datos de entrenamiento para ajustar el modelo. Este enfoque reduce el problema de estimar f a estimar un conjunto de par√°metros.

    nota: Una desventaja de este enfoque es que el modelo por lo general no coincide con la verdadera forma de f.

-   No param√©tricos: No se hace supociones de la forma funcional de f, se busca una estimaci√≥n de f que se acerque lo mejor posible a los puntos de datos sin ser demasiado ondulado. Evita la posibilidad que el modelo no se ajuste a los datos.

    nota: Una desventaja es que se requiere un gran numero de observaciones para obtener la estimaci√≥n precisas de f.

### La compesaci√≥n entre la precisi√≥n de la predicci√≥n y la interpretabilidad del modelo

La flexibilidad de un modelo se puede establecer en base de cuantas formas para estimar f puede producir, un ejemplo de modelo inflexible puede ser la regresi√≥n lineal debido a que solo genera funciones lineales.

Por otro lado existen otros metodos como plate splines son considerados flexibles ya que puede generar m√°s posibilidades de estimar f.

La selecci√≥n del modelo se basa en lo que se busca responder, por ejemplo si se desea obtener la inferencia un modelo m√°s restrictivo es m√°s interpetable.

### Aprendizaje supervisado vs no supervisado

En el aprendizaje estad√≠stico se tiene dos categor√≠as: supervisado o no supervisado.

En el aprendizaje supervisado cada observaci√≥n de las medidas predictoras tiene una medida de respuesta asociada. Se desea ajustar un modelo que relacione la respuesta con los predictores, con el objetivo de tener una mejor precisi√≥n en la respuesta para futuras observaciones (predicci√≥n) o comprender mejor la relaci√≥n entre la respuesta y los predictores (inferencia).

Por su parte el aprendizaje no supervisado describe una situaci√≥n donde para cada observaci√≥n se tiene un vector de medidas pero ninguna respuesta asociada, por lo cual no es posible ajustar un modelo de regresi√≥n lineal, ya que no hay una variable de respuesta que predecir.

#### Problemas de regresi√≥n vs clasificaci√≥n

Problemas de regresi√≥n: Problemas con respuestas cuantitativas (valores num√©ricos).

Problemas de clasificaci√≥n: Problemas con una respuesta cualitativa (categ√≥ricas).

Sin embargo hay que tomar en cuenta ciertos casos como:

La regresi√≥n lineal de m√≠nimos cuadrados tiene una respuesta cuantitativa mientras que la regresi√≥n log√≠stica usa una respuesta cualitativa.

### Evaluaci√≥n de la precisi√≥n del modelo

Debido a la naturaleza de los diferentes tipos de datos es necesario elegir que m√©todo produce el mejor resultado, esta puede ser una de las partes m√°s desafiantes.

#### Medici√≥n de la calidad de ajuste

Al evaluar el rendimiento de un m√©todo de aprendizaje estad√≠stico es necesario cuantificar hasta que punto el valor de respuesta pronosticado para una observaci√≥n se acerca al valor real de esa observaci√≥n. Dentro de la regresi√≥n se utiliza el error cuadr√°tico medio (MSE):

![](formula.png)

El MSE se calcula utilizando los datos de entrenamiento que se usaron al ajustar el modelo.

# Regresi√≥n lineal

Es un m√©todo de aprendizaje supervisado, se puede lograr predecir una respuesta cuantitativa, esto, con el objetivo de poder procesar dos o m√°s grupos de datos y encontrar una relaci√≥n entre ellos.

As√≠ tambi√©n, la regresi√≥n lineal se convierte en una herramienta √∫til al tener datos con una tendencia lineal de relaci√≥n logrando cuantificar su relaci√≥n de interacci√≥n.

### Regresi√≥n lineal simple

Es aplicada para predecir una respuesta cuantitativa *Y* a partir de una variable predictiva √∫nica *X,* esto suponiendo que existe una relaci√≥n casi lineal entre X y Y. Esto se puede representar en una ecuaci√≥n como:

$$Y ‚âà ùõΩ0 + ùõΩ1X + ‚àä$$

La ecuaci√≥n se representar√≠a como: "Regresi√≥n de Y sobre X", en donde ùõΩ0+ùõΩ1 son par√°metros desconocidos que representan la intersecci√≥n y la pendiente de los par√°metros. Una vez utilizados datos de entrenamiento para estimar ùõΩ0+ùõΩ1 se podr√° realiza una predicci√≥n de datos futuros.

### Estimaci√≥n de coeficientes ùõΩ0+ùõΩ1

El objetivo principal es obtener valores pertenecientes a cada uno de los coeficientes, tal que, se logre obtener o describir un modelo lineal, esto se logra mediante la representaci√≥n de "pares de observaciones" en las cuales se obtengan una medida de X y una medida de Y, eso con el objetivo de tener estimaciones para los coeficientes ùõΩ0, ùõΩ1 tal que el modelo se ajuste a los datos disponibles hasta encontrar una intersecci√≥n ùõΩ0 y una pendiente ùõΩ1, esto se lo puede lograr con el criterio de m√≠nimos cuadrados. Se muestra un ejemplo de la relaci√≥n lineal entre las ventas de una empresa y los gastos de publicidad en TV.

![Relacion lineal](Relacion%20lineal.png)

Es importante considerar el nivel de error en la aproximaci√≥n, el cual determina la posible variaci√≥n en el eje Y que hace que los datos no tengan un car√°cter lineal. Esto define la l√≠nea de regresi√≥n de la poblaci√≥n. Aplicando la estrategia de m√≠nimos cuadrados podemos obtener una nueva l√≠nea basada en los datos observados.

![Conjunto de datos](Conjunto%20de%20datos.png)

En la figura de la derecha se observa en color rojo la l√≠nea real de aproximaci√≥n, mientras en color negro la l√≠nea de m√≠nimos cuadrados en base a las observaciones, mientras que en la derecha se observan l√≠neas basadas en m√≠nimos cuadrados calculadas mediante observaciones aleatorias, estas estimaciones no se alejan demasiado de la regresi√≥n lineal de la poblaci√≥n (l√≠nea roja).

Como conclusi√≥n la regresi√≥n lineal simple es un enfoque √∫til para predecir una respuesta sobre la base de una √∫nica variable predictora.

### Regresi√≥n lineal m√∫ltiple

A diferencia de la regresi√≥n lineal simple, en la realidad existe m√°s de un predictor, por ello, en vez de realizar varias regresiones lineales simples en donde se sesgan varios datos se debe realizar una regresi√≥n lineal m√∫ltiple. Esto se puede representar en una ecuaci√≥n como:

$$
Y ‚âà ùõΩ_0 + ùõΩ_1X_1 + ùõΩ_2X_2+...+ ùõΩ_pX_p + ‚àä
$$

En donde X1, X2 ... Xp con √≠ndices predictores.

#### Estimaci√≥n de los coeficientes de regresi√≥n

Al igual que en la regresi√≥n lineal simple los coeficientes ùõΩ0, ùõΩ1 ... ùõΩp son los que deben estimarse y en base a estas estimaciones se pueden llagar a hacer predicciones. Para esta estimaci√≥n se utiliza el mismo enfoque de m√≠nimos cuadrados.

Para representar las estimaciones de coeficientes de regresi√≥n lineal m√∫ltiple se lo debe hacer mediante algebra matricial por medio de un paquete estad√≠stico.

Es necesario responder ciertas preguntas al realizar regresi√≥n lineal m√∫ltiple:

-   **¬øExiste alguna relaci√≥n entre la respuesta y los predictores?**

    En este caso se debe plantear una hip√≥tesis nula en donde ùõΩ0, ùõΩ1 ... ùõΩp = 0¬†y una hip√≥tesis alternativa, en donde ùõΩ0, ùõΩ1 ... ùõΩp ‚â† 0 , aplicando la f√≥rmula podemos obtener un falor de F en el cual se determina un valor cercano a 1 cuando no hay relaci√≥n entre la pregunta y la respuesta, mientras que si existiera relaci√≥n el valor ser√≠a superior a 1.

-   **Decidir sobre variables importantes**

    Al determinar que los predictores si tienen una relaci√≥n con las peguntas es necesario determinar cuales de ellos establecen esta relaci√≥n. Para ello, es necesario realizar una selecci√≥n de variables aplicando el m√©todo de Criterio de Informaci√≥n de Akaike u otro a conveniencia. Estos establecen 3 enfoques cl√°sicos:

-   Selecci√≥n hacia adelante: Implica iniciar con un modelo nulo, se ajustan regresiones lineales simples y se selecciona la variable con car√°cter mas bajo continuando con el ejercicio hasta cumplir con la regla de detenci√≥n.

-   Selecci√≥n hacia atr√°s: Se inicia con todas las variables, eliminando la variable con car√°cter mas alto, es decir, la menos significativa estad√≠sticamente.

-   Selecci√≥n mixta: es una combinaci√≥n de selecci√≥n hacia adelante y hacia atr√°s

![Grafica con 2 predictores](Dos%20predictores.png)

Se observa una grafica de los datos obtenidos, en donde existen 2 predictores y una sola respuesta, esto en un plano 3D.

Al ajustar un modelo a una regresi√≥n lineal pueden ocurrir varios problemas, entre los cuales est√°n:

-   **No linealidad de las relaciones respuesta-predictor:** Esto conlleva a que todas las conclusiones sean cuestionables y la precisi√≥n de predicci√≥n se reducen significativamente, esto se lo puede evitar aplicando los " gr√°ficos de residuos"

-   **Valores at√≠picos:** Estos valores surgen entre una de sus razones por el registro incorrecto de una observaci√≥n durante la recopilaci√≥n de datos, as√≠ mismo se pueden utilizar " gr√°ficos de residuos" para determinar el valor at√≠pico y ret√≠ralo, esto mejorar√° la ecuaci√≥n de linealidad obtenida.

-   **Puntos de apalancamiento:** son observaciones que se encuentran fuera del conglomerado de datos, esto produce que la ecuaci√≥n de linealidad tienda a flexionarse de acuerdo al nivel de apalancamiento de la observaci√≥n, mientras mas alejada este la observaci√≥n del conjunto de observaciones mayor grado de apalancamiento habr√°.

![Apalancamiento](apalancamiento.png)

Como se observa en la figura de la izquierda, la observaci√≥n 41 presenta un mayor grado de apalancamiento con respecto a la observaci√≥n 20, as√≠ mismo, en la figura central se evidencia una observaci√≥n inusual fuera del grupo de datos, esto produce un alto apalancamiento; mientras que en la figura de la derecha se observa un punto 41 con un grado muy alto de apalancamiento.

## Laboratorio 3

## Librer√≠as

Usamos la funci√≥n Library() para cargar librer√≠as, funciones o datasets que no est√©n incluidos en R, en este caso incluiremos las librer√≠as: MASS, que contiene una gran colecci√≥n de datasets y funciones, por otro lado, necesitaremos la librer√≠a ISLR2 que incluye datasets relacionados al libro gu√≠a.

```{r}
library(MASS)
library(ISLR2)
```

## Regresi√≥n Lineal Simple

Uno de los datasets es Boston que registra "medv" que es el valor medio de la casa, para 506 distritos de Boston; en este laboratorio se tratar√° de predecir medv usando par√°metros como rm (n√∫mero de promedio de habitaciones), age (edad de la casa), lstat (Porcentaje de hogares con bajo status econ√≥mico).

```{r}
head (Boston)
```

Utilizando la funci√≥n lm(), ajustaremos un modelo de regresi√≥n lineal simple definiendo los siguientes par√°metros: respuesta ser√° medv, el predictor ser√° Istat.

En una siguiente l√≠nea de c√≥digo, adjuntamos el dataset Boston de tal manera que la funci√≥n reconozca las variables.

```{r}
lm.fit <- lm(medv ~ lstat , data = Boston)
attach (Boston)
lm.fit <- lm(medv ~ lstat)
```

Para obtener m√°s informaci√≥n sobre el modelo utilizado, podemos digitar "lm.fit" o "summary(lm.fit)" para obtener informaci√≥n m√°s detallada sobre el modelo.

```{r}
lm.fit
summary(lm.fit)
```

As√≠ mismo, podemos llamar la funci√≥n "names()" para encontrar que infromaci√≥n se guard√≥ en lm.fit.

```{r}
names(lm.fit)
```

Por otro lado, podemos obtener el intervalo de confianza para la estimaci√≥n de los coeficientes utilizando el comando "confint()"

```{r}
confint(lm.fit)
```

La funci√≥n "predict", puede ser utilizada para producir intervalos de confianza y de predicci√≥n para medv dado un valor de lstat.

```{r}
predict (lm.fit , data.frame(lstat = (c(5, 10, 15))),
interval = "confidence")
predict (lm.fit , data.frame(lstat = (c(5, 10, 15))),
interval = "prediction")
```

Ahora trazaremos medv y lstat junto con una l√≠nea de regresi√≥n de m√≠nimos cuadrados con las funciones plot() y abline().

```{r}
plot(lstat, medv)
abline(lm.fit)
```

Ahora, la funci√≥n "abline()" puede dibujar cualquier l√≠nea, incluso, podemos dibujar una con intersecci√≥n y pendiente determinadas, solo necesitamos indicar estos par√°metros en el argumento de la funci√≥n, as√≠ mismo, podemos ser capaces de variar el ancho de la l√≠nea con el comando "lw=", as√≠ mismo, podemos especificar el color que deseamos para nuestra l√≠nea en espec√≠fico con el comando "col="; por otro lado usamos "pch" para crear diferentes s√≠mbolos de trazado.

```{r}
plot(lstat, medv)
abline (lm.fit , lwd = 3)
plot(lstat, medv)
abline (lm.fit , lwd = 3, col = " red ")
plot (lstat , medv , col = " red ")
plot (lstat , medv , pch = 20)
plot (lstat , medv , pch = "+")
plot (1:20, 1:20, pch = 1:20)
```

Mediante las funciones "par()" y "mfrow()" dividiremos la pantalla de visualizaci√≥n en paneles separados que permitan ver varios gr√°ficos simult√°neamente a elecci√≥n, esto nos servir√° para visualizar individualmente los diagramas de diagn√≥stico que se producen con la funci√≥n "plot()" directamente de la salida de "lm()".

```{r}
par (mfrow = c(2, 2))
plot (lm.fit)
```

As√≠ mismo, se puede calcular los residuos de un ajuste de regresi√≥n lineal usando la funci√≥n "residuals()"; la funci√≥n "rstudent()" devolver√° los residuos estudentizados, y podemos usar esta funci√≥n para graficar los residuos contra los valores ajustados.

```{r}
plot ( predict (lm.fit), residuals (lm.fit))
plot ( predict (lm.fit), rstudent (lm.fit))
```

En las gr√°ficas residuales, notamos que no existe linealidad; las estad√≠sticas de leverage pueden ser calculadas para cualquier n√∫mero de predictores mediante la funci√≥n "hatvalues()".

As√≠ mismo, la funci√≥n "which.max()" identifica el √≠ndice del elemento m√°s grande de un vectos, en este caso nos dir√° qu√© observaci√≥n tiene la mayor estad√≠stica de leverage.

```{r}
plot ( hatvalues (lm.fit))
which.max ( hatvalues (lm.fit))
```

## Regresi√≥n Lineal M√∫ltiple

Ahora, para ajustar un modelo de regresi√≥n lineal simple, nuevamente usamosd"lim()", pero con una sintaxis diferente, pues se utilizan 3 predictores; con la funci√≥n"summary()" se generar√°n los coeficientes de regresi√≥n para todos los predictores.

```{r}
lm.fit <- lm(medv ~ lstat + age , data = Boston)
summary (lm.fit)
```

El conjunto de datos de Boston contiene 12 variables, para generar una regresi√≥n lineal con todos estos se puede utilizar la siguiente abreviatura:

```{r}
lm.fit <- lm(medv ~ ., data = Boston)
summary (lm.fit)
```

Podemos acceder a los componentes individuales de un objeto de la funci√≥n "summary()" por su nombre, ingresando "?summary.lm" para ver qu√© hay disponible. Por lo tanto, summary(lm.fit)\$r.sq nos da el R2 (Coeficiente de determinaci√≥n), y summary(lm.fit)\$sigma nos da el RSE (error est√°ndar de la estimaci√≥n).

La funci√≥n vif() esparte del paquete "car", la utilizamos para calcular los factores de inflaci√≥n de la varianza. La mayor√≠a de los "VIF" son de bajos a moderados para estos datos. El paquete "car" no forma parte de la instalaci√≥n b√°sica de R, por lo debemos descargar la primera vez que lo usemos con el siguiente comando a trav√©s de la funci√≥n install.packages() en R.

```{r}
install.packages("car")
```

```{r}
library (car)
vif (lm.fit)
```

Ahora, si queremos hacer una regresi√≥n con todas las variables, excepto una, por ejemplo en la regresi√≥n realizada en el bloque anterior tiene un valor p alto, pues, utlizamos la siguiente sintaxis:

```{r}
lm.fit1 <- lm(medv ~ . - age , data = Boston)
summary (lm.fit1)
```

Para el mismo efecto, podemos utilizar la funci√≥n "update()":

```{r}
lm.fit1 <- update (lm.fit , ~ . - age)
```

## T√©rminos de interacci√≥n

Para un modelo lineal utilizando la funci√≥n "lm()", se pueden incluir t√©rminos de interacci√≥n entre lstat:black que le dice a R que incluya cierto t√©rmino de interacci√≥n como predictores, en este caso, se volvi√≥ a incluir como predictor a la variable "age":

```{r}
summary (lm(medv ~ lstat * age , data = Boston))
```

## Transformaciones no lineales de los predictores

Otra funci√≥n de "lm()" puede acomodar transformaciones no lineales de los predictores, por ejemplo, podemos crear un predictor X\^2 con "I(X\^2)", esta funci√≥n "I()" nos permite elevar X a la potencia 2, entonces, hacemos una regresi√≥n de medv en lstat y lstat2.

```{r}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary (lm.fit2)
```

Un valor p cercano a cero asociado con el t√©rmino cuadr√°tico sugiere que se tiene un modelo mejorado. Usando la funci√≥n "anova()" para cuantificar a√∫n m√°s la medida en que el ajuste cuadr√°tico es superior al ajuste lineal.

```{r}
lm.fit <- lm(medv ~ lstat)
anova (lm.fit , lm.fit2)
```

Aqu√≠ el modelo 1 representa el submodelo lineal que contiene un solo predictor que es lstat, por su parte Modelo 2 que corresponde a un modelo cuadr√°tico m√°s grande con dos predictores lstar y lstat2. La funci√≥n "anova()" realiza una prueba de hip√≥tesis comparando los dos modelos, la hip√≥tesis nula es que ambos modelos ajustan datos correctamente y la hip√≥tesis alternativa es que todo el modelo es superior. Aqu√≠ el estad√≠stico F es 135 y el valor p asociado es pr√°cticamente cero, evidenciando que el modelo que contiene ambos predictores, es muy superior al modelo que solo tiene un predictor, esto podemos relacionarlo con que, de acuerdo con lo que vimos la no linealidad en relaci√≥n entre "medv" y "lstat"

```{r}
par (mfrow = c(2, 2))
plot (lm.fit2)
```

Nos percatamos que cuando el t√©rmino lstat2 se incluye en el modelo hay un patr√≥n poco perceptible en los residuos. Para crear un ajuste c√∫bico se puede incluir un predictor de la forma "I(X\^3)", y podemos trabajarlo de esa manera para polinomios de orden superior, ahora, tambi√©n podemos utilizar la funci√≥n "poly()" para crear este polinomio dentro de "lm()", en el siguiente bloque de c√≥digo podemos ver su aplicaci√≥n para incluir un polinomio de quinto grado.

```{r}
lm.fit5 <- lm(medv ~ poly (lstat , 5))
summary (lm.fit5)
```

Este an√°lisis sugiere que incluir t√©rminos polin√≥micos adicionales, hasta el quinto orden, generan una mejora en el ajuste del modelo.

Por defecto, "poly()" ortogonaliza los predictores, esto significa que las caracter√≠sticas que genera esta funci√≥n no son simplemente una secuencia de potencias del argumento. Sin embargo, un modelo lineal aplicado a la salida de la funci√≥n "poly()" tendr√° los mismos valores ajustados que un modelo lineal aplicado a los polinomios sin procesar, ahora, para conseguirlos se deber√° usar el argumento "raw=TRUE"

```{r}
summary (lm(medv ~ log(rm), data = Boston))
```

## Predictores cualitativos

En la librer√≠a ISLR2 existe un dataset de "Carseats", a continuaci√≥n se intenta predecir las ventas de asientos de seguridad para ni√±os en 400 ubicaciones en funci√≥n de una serie de predictores.

Los datos de "carseats" incluyen predictores como "Shelveloc", un indicador de la calidad de la ubicaci√≥n de las estanter√≠as, tomando tres posibles valores: malo, medio y bueno.

Dada una variable cualitativa como Shelveloc, R genera autom√°ticamente variables ficticias. A continuaci√≥n ajustamos un modelo de regresi√≥n m√∫ltiple que incluye algunos t√©rminos de interacci√≥n.

```{r}
head (Carseats)
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age ,
data = Carseats)
summary (lm.fit)
```

La funci√≥n "contrasts()" devuelve la codificaci√≥n que usa R para la variable ficticia:

```{r}
attach (Carseats)
contrasts (ShelveLoc)
```

R ha creado una variable ficticia ShelveLocGood que toma el valor 1 si la ubicaci√≥n de la estanter√≠a es buena y 0 en caso contrario. Tambi√©n ha creado una variable ficticia ShelveLocMedium que equivale a 1 si la ubicaci√≥n de la estanter√≠a es mediana y 0 en caso contrario. Una ubicaci√≥n de estanter√≠a incorrecta corresponde a un cero para cada una de las dos variables ficticias. El hecho de que el coeficiente de ShelveLocGood en el resultado de la regresi√≥n sea positivo indica que una buena ubicaci√≥n de estanter√≠as est√° asociada con altas ventas (en relaci√≥n con una mala ubicaci√≥n).

## Escribiendo funciones

La funci√≥n "LoadLibraries()" es una funci√≥n en programaci√≥n que se utiliza para cargar librer√≠as o bibliotecas de c√≥digo en un programa o aplicaci√≥n, a continuaci√≥n escribiremos una funci√≥n simple que lea las bibliotecas ISLR2 y MASS:

```{r}
LoadLibraries <- function (){
+ library (ISLR2)
+ library (MASS)
+ print ("The libraries have been loaded .")
}
```

Ahora, si tipeamos "LoadLibraries", R nos dir√° qu√© hay dentro de una funci√≥n. Y si llamamos a la funci√≥n "LoadLibraries()" estaremos llamando a la funci√≥n.

```{r}
LoadLibraries
```

```{r}
#LoadLibraries()
```
